{
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "lastEditStatus": {
   "notebookId": "ya3gybjajkbtdbkovcb5",
   "authorId": "1758781709217",
   "authorName": "MCASTRO",
   "authorEmail": "marcel.castro@snowflake.com",
   "sessionId": "1c5a1784-3fdc-4d29-9ef5-007d53c1843e",
   "lastEditTime": 1756238527044
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22dae8-94e4-401e-b073-1cb4c236c530",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "---\n",
    "aliases:\n",
    "- /2025/08/26/SparkSnowflake\n",
    "badges: true\n",
    "categories:\n",
    "- spark\n",
    "- snowflake\n",
    "date: '2025-08-26'\n",
    "description: Details on how to run Apache Spark in Snowflake.\n",
    "output-file: 2025-08-26-sparkinSnowflake.html\n",
    "title: Running Apache Spark in Snowflake\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a00a7-4030-497d-866b-a4e293881463",
   "metadata": {
    "collapsed": false,
    "name": "topic"
   },
   "source": "# Running Apache Spark in Snowflake\n\nDataset: https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv\n\n![image-20240507155528488](./images/image-sparksnowflake.png)\n\ntest\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733f4855-7791-460d-aaa2-43f2b615f6b3",
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Connection failed: 250001 (08001): Failed to connect to DB: SFSEEUROPE-MCASTRO_AWS1_USWEST2.snowflakecomputing.com:443. JWT token is invalid. [715fe50c-9611-443c-a239-b7f2b520c157]\n",
      "Please check your Snowflake credentials in snowflake_config.py or .env file\n"
     ]
    }
   ],
   "source": [
    "# Snowflake Connection Setup for Cursor/Jupyter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from snowflake.snowpark import Session\n",
    "# from snowflake import snowpark_connect  # Not available in standard installations\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Connection parameters - Update these with your Snowflake credentials\n",
    "# connection_parameters = {\n",
    "#     'account': os.getenv('SNOWFLAKE_ACCOUNT', 'your_account_identifier'),\n",
    "#     'user': os.getenv('SNOWFLAKE_USER', 'your_username'), \n",
    "#     'password': os.getenv('SNOWFLAKE_PASSWORD', 'your_password'),\n",
    "#     'role': os.getenv('SNOWFLAKE_ROLE', 'SYSADMIN'),\n",
    "#     'warehouse': os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "#     'database': os.getenv('SNOWFLAKE_DATABASE', 'AICOLLEGE'),\n",
    "#     'schema': os.getenv('SNOWFLAKE_SCHEMA', 'PUBLIC')\n",
    "# }\n",
    "\n",
    "# Alternative: Import from config file\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Try to import snowflake_config from different possible locations\n",
    "try:\n",
    "    from snowflake_config import SNOWFLAKE_CONFIG\n",
    "    connection_parameters = SNOWFLAKE_CONFIG\n",
    "except ImportError:\n",
    "    # If direct import fails, try adding parent directories to path\n",
    "    current_dir = os.getcwd()\n",
    "    parent_dirs = [\n",
    "        current_dir,\n",
    "        os.path.dirname(current_dir),\n",
    "        os.path.dirname(os.path.dirname(current_dir))\n",
    "    ]\n",
    "    \n",
    "    config_found = False\n",
    "    for dir_path in parent_dirs:\n",
    "        config_path = os.path.join(dir_path, 'snowflake_config.py')\n",
    "        if os.path.exists(config_path):\n",
    "            sys.path.insert(0, dir_path)\n",
    "            try:\n",
    "                from snowflake_config import SNOWFLAKE_CONFIG\n",
    "                connection_parameters = SNOWFLAKE_CONFIG\n",
    "                config_found = True\n",
    "                print(f\"✅ Found snowflake_config.py in: {dir_path}\")\n",
    "                break\n",
    "            except ImportError:\n",
    "                continue\n",
    "    \n",
    "    if not config_found:\n",
    "        print(\"❌ Could not find snowflake_config.py\")\n",
    "        # Fallback to direct configuration with proper private key loading\n",
    "        from cryptography.hazmat.primitives import serialization\n",
    "        \n",
    "        def load_private_key_fallback(private_key_path):\n",
    "            try:\n",
    "                expanded_path = os.path.expanduser(private_key_path)\n",
    "                if not os.path.exists(expanded_path):\n",
    "                    print(f\"❌ Private key file not found: {expanded_path}\")\n",
    "                    return None\n",
    "                    \n",
    "                with open(expanded_path, 'rb') as key_file:\n",
    "                    private_key = serialization.load_pem_private_key(\n",
    "                        key_file.read(),\n",
    "                        password=None\n",
    "                    )\n",
    "                \n",
    "                private_key_bytes = private_key.private_bytes(\n",
    "                    encoding=serialization.Encoding.DER,\n",
    "                    format=serialization.PrivateFormat.PKCS8,\n",
    "                    encryption_algorithm=serialization.NoEncryption()\n",
    "                )\n",
    "                return private_key_bytes\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading private key: {e}\")\n",
    "                return None\n",
    "        \n",
    "        private_key_bytes = load_private_key_fallback('~/.ssh/mlops_hol_rsa_private_key.pem')\n",
    "        \n",
    "        connection_parameters = {\n",
    "            'account': 'SFSEEUROPE-MCASTRO_AWS1_USWEST2',\n",
    "            'user': 'mlops_user',\n",
    "            'role': 'aicollege',\n",
    "            'warehouse': 'aicollege',\n",
    "            'database': 'aicollege',\n",
    "            'schema': 'PUBLIC',\n",
    "            'authenticator': 'SNOWFLAKE_JWT',\n",
    "            'private_key': private_key_bytes\n",
    "        }\n",
    "\n",
    "try:\n",
    "    # Create Snowpark session\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    \n",
    "    # Note: We'll use Snowpark DataFrames instead of Spark DataFrames\n",
    "    # Snowpark provides similar functionality to Spark for data processing\n",
    "    \n",
    "    print(\"✅ Connected to Snowflake successfully!\")\n",
    "    print(f\"Current database: {session.get_current_database()}\")\n",
    "    print(f\"Current schema: {session.get_current_schema()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {str(e)}\")\n",
    "    print(\"Please check your Snowflake credentials in snowflake_config.py or .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af9254-da40-4e5a-9525-8d3a34b379ea",
   "metadata": {
    "language": "python",
    "name": "setup"
   },
   "outputs": [],
   "source": [
    "# Set up the env for Java libraries and enable the Spark Connect Mode\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "os.environ['JAVA_HOME'] = os.environ[\"CONDA_PREFIX\"]\n",
    "os.environ['JAVA_LD_LIBRARY_PATH'] = os.path.join(os.environ[\"CONDA_PREFIX\"], 'lib', 'server')\n",
    "os.environ[\"SPARK_LOCAL_HOSTNAME\"] = \"127.0.0.1\"\n",
    "os.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"1\"\n",
    "\n",
    "from snowflake import snowpark_connect\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "\n",
    "session = get_active_session()\n",
    "snowpark_connect.start_session(snowpark_session = session)\n",
    "\n",
    "\n",
    "# Here is your normal pyspark code. You can of course have them in other Python Cells\n",
    "spark = snowpark_connect.get_session()\n",
    "df = spark.sql(\"show schemas\").limit(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc925d72-924d-4f3b-aca9-9fbdb0cf7b9f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "create_dataframe"
   },
   "outputs": [],
   "source": [
    "data = [[2021, \"test\", \"Albany\", \"M\", 42]]\n",
    "columns = [\"Year\", \"First_Name\", \"County\", \"Sex\", \"Count\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema=\"Year int, First_Name STRING, County STRING, Sex STRING, Count int\")\n",
    "#display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.\n",
    "df1.show() #The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82578820-f654-411d-91e2-ca422fc610dd",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "load_csv"
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(f\"@aicollege.public.setup/row.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\")\n",
    "#display(df_csv)\n",
    "df_csv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d1b48-c43d-4074-b4c7-a31e6277d917",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "interact_with_dataframe"
   },
   "outputs": [],
   "source": [
    "df_csv.printSchema()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972727b-6c8d-4f6d-969c-51e19a4a4bfa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "rename"
   },
   "outputs": [],
   "source": [
    "df_csv = df_csv.withColumnRenamed(\"First Name\", \"First_Name\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab83992-8170-490e-9632-3408d172d8a1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "combine"
   },
   "outputs": [],
   "source": [
    "df = df1.union(df_csv)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3644da-d810-4680-ad0d-b9d580bfa016",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "filter"
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"Count\"] > 50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb7728-6dd4-4c5a-85cf-902903d82b9f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "where"
   },
   "outputs": [],
   "source": [
    "df.where(df[\"Count\"] > 50).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5f38-bf7f-49ad-b7da-a4a98cc2c770",
   "metadata": {
    "language": "python",
    "name": "select_column"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df.select(\"First_Name\", \"Count\").orderBy(desc(\"Count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f848a-082b-4237-94ae-8209fd27abf6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "subset_df"
   },
   "outputs": [],
   "source": [
    "subsetDF = df.filter((df[\"Year\"] == 2009) & (df[\"Count\"] > 100) & (df[\"Sex\"] == \"F\")).select(\"First_Name\", \"County\", \"Count\").orderBy(desc(\"Count\"))\n",
    "subsetDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88964d4a-e3a7-400e-ac7e-f1b5de990077",
   "metadata": {
    "language": "python",
    "name": "save_df"
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"AICOLLEGE.PUBLIC.MYFIRSTSPARK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98171cb6-6e9c-4adf-8248-bae1a4638068",
   "metadata": {
    "language": "python",
    "name": "save_to_json"
   },
   "outputs": [],
   "source": [
    "df.write.format(\"json\").mode(\"overwrite\").save(f\"@aicollege.public.setup/myfirstspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10945fd0-e03e-4f4e-a172-5b62055c6e74",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "read_json"
   },
   "outputs": [],
   "source": [
    "#spark.read.format(\"json\").json(f\"@aicollege.public.setup/myfirstspark\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bac19c-6ab6-40e7-bbb7-b46cbab0d2c8",
   "metadata": {
    "language": "python",
    "name": "selectExpr"
   },
   "outputs": [],
   "source": [
    "df.selectExpr(\"Count\", \"upper(County) as big_name\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7565d0-1708-4f33-918f-f6d2669e359b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "expr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.select(\"Count\", expr(\"lower(County) as little_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af7cf4-4023-42b0-acb9-4cd11189c80e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "spark_sql"
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM AICOLLEGE.PUBLIC.MYFIRSTSPARK\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ab187-598b-4f32-9456-b819c315ab51",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": []
  }
 ]
}